{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "#!pip install -r /work/NLP_IMDb_Exam/requirements.txt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import datasets\n",
    "import evaluate\n",
    "import seaborn as sns\n",
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, AutoModel\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def clean_string(raw_string):\n",
    "    \"\"\"\n",
    "    Takes in a raw_string and returns the cleaned version.\n",
    "    \"\"\"\n",
    "    clean_text = re.sub(r'<br/><br/>', ' ', raw_string)  # Remove breaks\n",
    "    clean_text = re.sub(r'/', ' ', clean_text)  # Remove slashes   \n",
    "    clean_text = re.sub(r'[^a-zA-Z0-9 ]', '', clean_text).lower()  # Remove special characters and lowercase\n",
    "    return clean_text\n",
    "\n",
    "def save_embedding_with_information(embedding, IMDb_subset, filename):\n",
    "    # Save embedding for future:\n",
    "    embedding = pd.DataFrame(embedding) # Pandas dataframe of embeddings\n",
    "    embedding[['rating', 'average_rating']] = IMDb_subset[['rating', 'average_rating']].apply(pd.to_numeric)\n",
    "    embedding['review'] = IMDb_subset['review']\n",
    "    embedding.to_csv(f'../Data/{filename}/{filename}.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dataset:\n",
    "IMDb_dataframe = pd.read_csv('/work/NLP_IMDb_Exam/Data/review_dataframe.csv')\n",
    "IMDb_dataframe = IMDb_dataframe[IMDb_dataframe['rating'] != \"Null\"] # Remove nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMDb_subset = IMDb_dataframe.sample(n=100000, random_state=42, ignore_index=True)\n",
    "IMDb_subset['review'] = IMDb_subset['review'].apply(clean_string)\n",
    "IMDb_subset.to_csv('../Data/review_dataframe_subset.csv', index=False)\n",
    "subset_corpus= IMDb_subset['review'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "# Prepare models:\n",
    "# Load best small model for sentence embedding\n",
    "Mini_Model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\", device=\"cuda\")\n",
    "# Load best basic model for sentence embedding:\n",
    "MPNET_Model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2', device=\"cuda\") # Choose the best sentence transformer according to https://www.sbert.net/docs/sentence_transformer/pretrained_models.html\n",
    "# Load instructor model, for giving a promt for embedding:\n",
    "Instructor_Model = SentenceTransformer(\"hkunlp/instructor-large\", device=\"cuda\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embed the mini model:\n",
    "Mini_Embedding= Mini_Model.encode(subset_corpus)\n",
    "save_embedding_with_information(Mini_Embedding, IMDb_subset, \"MiniLM_L6\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "MPNET_Embedding = MPNET_Model.encode(subset_corpus)\n",
    "save_embedding_with_information(MPNET_Embedding, IMDb_subset, \"MPNET_base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "Instructor_Embedding = Instructor_Model.encode(\n",
    "    subset_corpus,\n",
    "    prompt=\"Represent the movie review for classifying the corresponding movie rating: \",\n",
    ")\n",
    "save_embedding_with_information(Instructor_Embedding, IMDb_subset, \"Instructor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all models\n",
    "filenames = [\"IMDb_Embedding-MiniLM_L6\", \"IMDb_Embedding-MPNET_base\", \"IMDb_Embdding_Instructor_rating\"]\n",
    "embeddings = [Mini_Embedding, MPNET_Embedding, Instructor_Embedding]\n",
    "\n",
    "for filename, embedding in zip(filenames, embeddings):\n",
    "    save_embedding_with_information(embedding=embedding, IMDb_subset=IMDb_subset, filename=filename)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
