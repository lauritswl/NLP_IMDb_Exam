{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting datasets (from -r /work/NLP_IMDb_Exam/requirements.txt (line 1))\n",
      "  Downloading datasets-3.2.0-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting evaluate (from -r /work/NLP_IMDb_Exam/requirements.txt (line 2))\n",
      "  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\n",
      "Collecting matplotlib (from -r /work/NLP_IMDb_Exam/requirements.txt (line 3))\n",
      "  Downloading matplotlib-3.10.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
      "Collecting numpy (from -r /work/NLP_IMDb_Exam/requirements.txt (line 4))\n",
      "  Downloading numpy-2.2.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
      "Collecting pandas (from -r /work/NLP_IMDb_Exam/requirements.txt (line 5))\n",
      "  Downloading pandas-2.2.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)\n",
      "Collecting scikit-learn (from -r /work/NLP_IMDb_Exam/requirements.txt (line 6))\n",
      "  Downloading scikit_learn-1.6.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
      "Collecting scipy (from -r /work/NLP_IMDb_Exam/requirements.txt (line 7))\n",
      "  Downloading scipy-1.14.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
      "Collecting seaborn (from -r /work/NLP_IMDb_Exam/requirements.txt (line 8))\n",
      "  Downloading seaborn-0.13.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting sentence_transformers (from -r /work/NLP_IMDb_Exam/requirements.txt (line 9))\n",
      "  Downloading sentence_transformers-3.3.1-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting tensorflow (from -r /work/NLP_IMDb_Exam/requirements.txt (line 10))\n",
      "  Downloading tensorflow-2.18.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
      "Collecting tf_keras (from -r /work/NLP_IMDb_Exam/requirements.txt (line 11))\n",
      "  Downloading tf_keras-2.18.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting torch (from -r /work/NLP_IMDb_Exam/requirements.txt (line 12))\n",
      "  Downloading torch-2.5.1-cp312-cp312-manylinux1_x86_64.whl.metadata (28 kB)\n",
      "Collecting transformers (from -r /work/NLP_IMDb_Exam/requirements.txt (line 14))\n",
      "  Downloading transformers-4.47.1-py3-none-any.whl.metadata (44 kB)\n",
      "Collecting statsmodels (from -r /work/NLP_IMDb_Exam/requirements.txt (line 15))\n",
      "  Downloading statsmodels-0.14.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.2 kB)\n",
      "Collecting filelock (from datasets->-r /work/NLP_IMDb_Exam/requirements.txt (line 1))\n",
      "  Downloading filelock-3.16.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting pyarrow>=15.0.0 (from datasets->-r /work/NLP_IMDb_Exam/requirements.txt (line 1))\n",
      "  Downloading pyarrow-18.1.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets->-r /work/NLP_IMDb_Exam/requirements.txt (line 1))\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: requests>=2.32.2 in /home/ucloud/.local/lib/python3.12/site-packages (from datasets->-r /work/NLP_IMDb_Exam/requirements.txt (line 1)) (2.32.3)\n",
      "Collecting tqdm>=4.66.3 (from datasets->-r /work/NLP_IMDb_Exam/requirements.txt (line 1))\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting xxhash (from datasets->-r /work/NLP_IMDb_Exam/requirements.txt (line 1))\n",
      "  Downloading xxhash-3.5.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets->-r /work/NLP_IMDb_Exam/requirements.txt (line 1))\n",
      "  Downloading multiprocess-0.70.16-py312-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec<=2024.9.0,>=2023.1.0 (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets->-r /work/NLP_IMDb_Exam/requirements.txt (line 1))\n",
      "  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting aiohttp (from datasets->-r /work/NLP_IMDb_Exam/requirements.txt (line 1))\n",
      "  Downloading aiohttp-3.11.11-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Collecting huggingface-hub>=0.23.0 (from datasets->-r /work/NLP_IMDb_Exam/requirements.txt (line 1))\n",
      "  Downloading huggingface_hub-0.27.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: packaging in /home/ucloud/.local/lib/python3.12/site-packages (from datasets->-r /work/NLP_IMDb_Exam/requirements.txt (line 1)) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ucloud/.local/lib/python3.12/site-packages (from datasets->-r /work/NLP_IMDb_Exam/requirements.txt (line 1)) (6.0.2)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib->-r /work/NLP_IMDb_Exam/requirements.txt (line 3))\n",
      "  Downloading contourpy-1.3.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.4 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib->-r /work/NLP_IMDb_Exam/requirements.txt (line 3))\n",
      "  Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib->-r /work/NLP_IMDb_Exam/requirements.txt (line 3))\n",
      "  Downloading fonttools-4.55.3-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (165 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib->-r /work/NLP_IMDb_Exam/requirements.txt (line 3))\n",
      "  Downloading kiwisolver-1.4.8-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.2 kB)\n",
      "Collecting pillow>=8 (from matplotlib->-r /work/NLP_IMDb_Exam/requirements.txt (line 3))\n",
      "  Downloading pillow-11.0.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (9.1 kB)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/lib/python3/dist-packages (from matplotlib->-r /work/NLP_IMDb_Exam/requirements.txt (line 3)) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/ucloud/.local/lib/python3.12/site-packages (from matplotlib->-r /work/NLP_IMDb_Exam/requirements.txt (line 3)) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas->-r /work/NLP_IMDb_Exam/requirements.txt (line 5))\n",
      "  Downloading pytz-2024.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas->-r /work/NLP_IMDb_Exam/requirements.txt (line 5))\n",
      "  Downloading tzdata-2024.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn->-r /work/NLP_IMDb_Exam/requirements.txt (line 6))\n",
      "  Downloading joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn->-r /work/NLP_IMDb_Exam/requirements.txt (line 6))\n",
      "  Downloading threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting absl-py>=1.0.0 (from tensorflow->-r /work/NLP_IMDb_Exam/requirements.txt (line 10))\n",
      "  Downloading absl_py-2.1.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow->-r /work/NLP_IMDb_Exam/requirements.txt (line 10))\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting flatbuffers>=24.3.25 (from tensorflow->-r /work/NLP_IMDb_Exam/requirements.txt (line 10))\n",
      "  Downloading flatbuffers-24.12.23-py2.py3-none-any.whl.metadata (876 bytes)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow->-r /work/NLP_IMDb_Exam/requirements.txt (line 10))\n",
      "  Downloading gast-0.6.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting google-pasta>=0.1.1 (from tensorflow->-r /work/NLP_IMDb_Exam/requirements.txt (line 10))\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
      "Collecting libclang>=13.0.0 (from tensorflow->-r /work/NLP_IMDb_Exam/requirements.txt (line 10))\n",
      "  Downloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl.metadata (5.2 kB)\n",
      "Collecting opt-einsum>=2.3.2 (from tensorflow->-r /work/NLP_IMDb_Exam/requirements.txt (line 10))\n",
      "  Downloading opt_einsum-3.4.0-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 (from tensorflow->-r /work/NLP_IMDb_Exam/requirements.txt (line 10))\n",
      "  Downloading protobuf-5.29.2-cp38-abi3-manylinux2014_x86_64.whl.metadata (592 bytes)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from tensorflow->-r /work/NLP_IMDb_Exam/requirements.txt (line 10)) (68.1.2)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/lib/python3/dist-packages (from tensorflow->-r /work/NLP_IMDb_Exam/requirements.txt (line 10)) (1.16.0)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow->-r /work/NLP_IMDb_Exam/requirements.txt (line 10))\n",
      "  Downloading termcolor-2.5.0-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting typing-extensions>=3.6.6 (from tensorflow->-r /work/NLP_IMDb_Exam/requirements.txt (line 10))\n",
      "  Downloading typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting wrapt>=1.11.0 (from tensorflow->-r /work/NLP_IMDb_Exam/requirements.txt (line 10))\n",
      "  Downloading wrapt-1.17.0-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.4 kB)\n",
      "Collecting grpcio<2.0,>=1.24.3 (from tensorflow->-r /work/NLP_IMDb_Exam/requirements.txt (line 10))\n",
      "  Downloading grpcio-1.68.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.9 kB)\n",
      "Collecting tensorboard<2.19,>=2.18 (from tensorflow->-r /work/NLP_IMDb_Exam/requirements.txt (line 10))\n",
      "  Downloading tensorboard-2.18.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting keras>=3.5.0 (from tensorflow->-r /work/NLP_IMDb_Exam/requirements.txt (line 10))\n",
      "  Downloading keras-3.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting numpy (from -r /work/NLP_IMDb_Exam/requirements.txt (line 4))\n",
      "  Downloading numpy-2.0.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
      "Collecting h5py>=3.11.0 (from tensorflow->-r /work/NLP_IMDb_Exam/requirements.txt (line 10))\n",
      "  Downloading h5py-3.12.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.5 kB)\n",
      "Collecting ml-dtypes<0.5.0,>=0.4.0 (from tensorflow->-r /work/NLP_IMDb_Exam/requirements.txt (line 10))\n",
      "  Downloading ml_dtypes-0.4.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
      "Collecting networkx (from torch->-r /work/NLP_IMDb_Exam/requirements.txt (line 12))\n",
      "  Downloading networkx-3.4.2-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: jinja2 in /home/ucloud/.local/lib/python3.12/site-packages (from torch->-r /work/NLP_IMDb_Exam/requirements.txt (line 12)) (3.1.4)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch->-r /work/NLP_IMDb_Exam/requirements.txt (line 12))\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch->-r /work/NLP_IMDb_Exam/requirements.txt (line 12))\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch->-r /work/NLP_IMDb_Exam/requirements.txt (line 12))\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch->-r /work/NLP_IMDb_Exam/requirements.txt (line 12))\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch->-r /work/NLP_IMDb_Exam/requirements.txt (line 12))\n",
      "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch->-r /work/NLP_IMDb_Exam/requirements.txt (line 12))\n",
      "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.5.147 (from torch->-r /work/NLP_IMDb_Exam/requirements.txt (line 12))\n",
      "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch->-r /work/NLP_IMDb_Exam/requirements.txt (line 12))\n",
      "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch->-r /work/NLP_IMDb_Exam/requirements.txt (line 12))\n",
      "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-nccl-cu12==2.21.5 (from torch->-r /work/NLP_IMDb_Exam/requirements.txt (line 12))\n",
      "  Downloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.4.127 (from torch->-r /work/NLP_IMDb_Exam/requirements.txt (line 12))\n",
      "  Downloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch->-r /work/NLP_IMDb_Exam/requirements.txt (line 12))\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting triton==3.1.0 (from torch->-r /work/NLP_IMDb_Exam/requirements.txt (line 12))\n",
      "  Downloading triton-3.1.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.3 kB)\n",
      "Collecting sympy==1.13.1 (from torch->-r /work/NLP_IMDb_Exam/requirements.txt (line 12))\n",
      "  Downloading sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy==1.13.1->torch->-r /work/NLP_IMDb_Exam/requirements.txt (line 12))\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting regex!=2019.12.17 (from transformers[torch]->-r /work/NLP_IMDb_Exam/requirements.txt (line 13))\n",
      "  Downloading regex-2024.11.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers[torch]->-r /work/NLP_IMDb_Exam/requirements.txt (line 13))\n",
      "  Downloading tokenizers-0.21.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers[torch]->-r /work/NLP_IMDb_Exam/requirements.txt (line 13))\n",
      "  Downloading safetensors-0.4.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting accelerate>=0.26.0 (from transformers[torch]->-r /work/NLP_IMDb_Exam/requirements.txt (line 13))\n",
      "  Downloading accelerate-1.2.1-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting patsy>=0.5.6 (from statsmodels->-r /work/NLP_IMDb_Exam/requirements.txt (line 15))\n",
      "  Downloading patsy-1.0.1-py2.py3-none-any.whl.metadata (3.3 kB)\n",
      "Requirement already satisfied: psutil in /home/ucloud/.local/lib/python3.12/site-packages (from accelerate>=0.26.0->transformers[torch]->-r /work/NLP_IMDb_Exam/requirements.txt (line 13)) (6.1.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/lib/python3/dist-packages (from astunparse>=1.6.0->tensorflow->-r /work/NLP_IMDb_Exam/requirements.txt (line 10)) (0.42.0)\n",
      "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp->datasets->-r /work/NLP_IMDb_Exam/requirements.txt (line 1))\n",
      "  Downloading aiohappyeyeballs-2.4.4-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->datasets->-r /work/NLP_IMDb_Exam/requirements.txt (line 1))\n",
      "  Downloading aiosignal-1.3.2-py2.py3-none-any.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/ucloud/.local/lib/python3.12/site-packages (from aiohttp->datasets->-r /work/NLP_IMDb_Exam/requirements.txt (line 1)) (24.2.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->datasets->-r /work/NLP_IMDb_Exam/requirements.txt (line 1))\n",
      "  Downloading frozenlist-1.5.0-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets->-r /work/NLP_IMDb_Exam/requirements.txt (line 1))\n",
      "  Downloading multidict-6.1.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.0 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp->datasets->-r /work/NLP_IMDb_Exam/requirements.txt (line 1))\n",
      "  Downloading propcache-0.2.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.2 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp->datasets->-r /work/NLP_IMDb_Exam/requirements.txt (line 1))\n",
      "  Downloading yarl-1.18.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (69 kB)\n",
      "Collecting rich (from keras>=3.5.0->tensorflow->-r /work/NLP_IMDb_Exam/requirements.txt (line 10))\n",
      "  Downloading rich-13.9.4-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting namex (from keras>=3.5.0->tensorflow->-r /work/NLP_IMDb_Exam/requirements.txt (line 10))\n",
      "  Downloading namex-0.0.8-py3-none-any.whl.metadata (246 bytes)\n",
      "Collecting optree (from keras>=3.5.0->tensorflow->-r /work/NLP_IMDb_Exam/requirements.txt (line 10))\n",
      "  Downloading optree-0.13.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (47 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ucloud/.local/lib/python3.12/site-packages (from requests>=2.32.2->datasets->-r /work/NLP_IMDb_Exam/requirements.txt (line 1)) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ucloud/.local/lib/python3.12/site-packages (from requests>=2.32.2->datasets->-r /work/NLP_IMDb_Exam/requirements.txt (line 1)) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ucloud/.local/lib/python3.12/site-packages (from requests>=2.32.2->datasets->-r /work/NLP_IMDb_Exam/requirements.txt (line 1)) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ucloud/.local/lib/python3.12/site-packages (from requests>=2.32.2->datasets->-r /work/NLP_IMDb_Exam/requirements.txt (line 1)) (2024.8.30)\n",
      "Collecting markdown>=2.6.8 (from tensorboard<2.19,>=2.18->tensorflow->-r /work/NLP_IMDb_Exam/requirements.txt (line 10))\n",
      "  Downloading Markdown-3.7-py3-none-any.whl.metadata (7.0 kB)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard<2.19,>=2.18->tensorflow->-r /work/NLP_IMDb_Exam/requirements.txt (line 10))\n",
      "  Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl.metadata (1.1 kB)\n",
      "Collecting werkzeug>=1.0.1 (from tensorboard<2.19,>=2.18->tensorflow->-r /work/NLP_IMDb_Exam/requirements.txt (line 10))\n",
      "  Downloading werkzeug-3.1.3-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ucloud/.local/lib/python3.12/site-packages (from jinja2->torch->-r /work/NLP_IMDb_Exam/requirements.txt (line 12)) (3.0.2)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich->keras>=3.5.0->tensorflow->-r /work/NLP_IMDb_Exam/requirements.txt (line 10))\n",
      "  Downloading markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/ucloud/.local/lib/python3.12/site-packages (from rich->keras>=3.5.0->tensorflow->-r /work/NLP_IMDb_Exam/requirements.txt (line 10)) (2.18.0)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow->-r /work/NLP_IMDb_Exam/requirements.txt (line 10))\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Downloading datasets-3.2.0-py3-none-any.whl (480 kB)\n",
      "Downloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n",
      "Downloading matplotlib-3.10.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m34.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pandas-2.2.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.7/12.7 MB\u001b[0m \u001b[31m26.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading scikit_learn-1.6.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m26.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading scipy-1.14.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (40.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.8/40.8 MB\u001b[0m \u001b[31m50.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hDownloading seaborn-0.13.2-py3-none-any.whl (294 kB)\n",
      "Downloading sentence_transformers-3.3.1-py3-none-any.whl (268 kB)\n",
      "Downloading tensorflow-2.18.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (615.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m615.5/615.5 MB\u001b[0m \u001b[31m66.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading numpy-2.0.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (19.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.2/19.2 MB\u001b[0m \u001b[31m40.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tf_keras-2.18.0-py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading torch-2.5.1-cp312-cp312-manylinux1_x86_64.whl (906.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m906.4/906.4 MB\u001b[0m \u001b[31m57.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m86.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m45.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m49.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m59.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m64.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m37.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m58.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m74.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m78.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (99 kB)\n",
      "Downloading sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading triton-3.1.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (209.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.6/209.6 MB\u001b[0m \u001b[31m76.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading transformers-4.47.1-py3-none-any.whl (10.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.1/10.1 MB\u001b[0m \u001b[31m27.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading statsmodels-0.14.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.7/10.7 MB\u001b[0m \u001b[31m28.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
      "Downloading accelerate-1.2.1-py3-none-any.whl (336 kB)\n",
      "Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Downloading contourpy-1.3.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (323 kB)\n",
      "Downloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Downloading flatbuffers-24.12.23-py2.py3-none-any.whl (30 kB)\n",
      "Downloading fonttools-4.55.3-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m29.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
      "Downloading aiohttp-3.11.11-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading gast-0.6.0-py3-none-any.whl (21 kB)\n",
      "Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Downloading grpcio-1.68.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading h5py-3.12.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m26.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.27.0-py3-none-any.whl (450 kB)\n",
      "Downloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Downloading keras-3.7.0-py3-none-any.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading kiwisolver-1.4.8-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl (24.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.5/24.5 MB\u001b[0m \u001b[31m54.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading ml_dtypes-0.4.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading multiprocess-0.70.16-py312-none-any.whl (146 kB)\n",
      "Downloading opt_einsum-3.4.0-py3-none-any.whl (71 kB)\n",
      "Downloading patsy-1.0.1-py2.py3-none-any.whl (232 kB)\n",
      "Downloading pillow-11.0.0-cp312-cp312-manylinux_2_28_x86_64.whl (4.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m28.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading protobuf-5.29.2-cp38-abi3-manylinux2014_x86_64.whl (319 kB)\n",
      "Downloading pyarrow-18.1.0-cp312-cp312-manylinux_2_28_x86_64.whl (40.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.1/40.1 MB\u001b[0m \u001b[31m54.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading pytz-2024.2-py2.py3-none-any.whl (508 kB)\n",
      "Downloading regex-2024.11.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (796 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m796.9/796.9 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.4.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (434 kB)\n",
      "Downloading tensorboard-2.18.0-py3-none-any.whl (5.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m36.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading termcolor-2.5.0-py3-none-any.whl (7.8 kB)\n",
      "Downloading threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Downloading tokenizers-0.21.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m24.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Downloading typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
      "Downloading tzdata-2024.2-py2.py3-none-any.whl (346 kB)\n",
      "Downloading wrapt-1.17.0-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (89 kB)\n",
      "Downloading filelock-3.16.1-py3-none-any.whl (16 kB)\n",
      "Downloading networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading xxhash-3.5.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "Downloading aiohappyeyeballs-2.4.4-py3-none-any.whl (14 kB)\n",
      "Downloading aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\n",
      "Downloading frozenlist-1.5.0-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (283 kB)\n",
      "Downloading Markdown-3.7-py3-none-any.whl (106 kB)\n",
      "Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading multidict-6.1.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (131 kB)\n",
      "Downloading propcache-0.2.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (243 kB)\n",
      "Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m27.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading werkzeug-3.1.3-py3-none-any.whl (224 kB)\n",
      "Downloading yarl-1.18.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (336 kB)\n",
      "Downloading namex-0.0.8-py3-none-any.whl (5.8 kB)\n",
      "Downloading optree-0.13.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (385 kB)\n",
      "Downloading rich-13.9.4-py3-none-any.whl (242 kB)\n",
      "Downloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: pytz, namex, mpmath, libclang, flatbuffers, xxhash, wrapt, werkzeug, tzdata, typing-extensions, tqdm, threadpoolctl, termcolor, tensorboard-data-server, sympy, safetensors, regex, pyarrow, protobuf, propcache, pillow, opt-einsum, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, networkx, multidict, mdurl, markdown, kiwisolver, joblib, grpcio, google-pasta, gast, fsspec, frozenlist, fonttools, filelock, dill, cycler, astunparse, aiohappyeyeballs, absl-py, yarl, triton, tensorboard, scipy, patsy, pandas, optree, nvidia-cusparse-cu12, nvidia-cudnn-cu12, multiprocess, ml-dtypes, markdown-it-py, huggingface-hub, h5py, contourpy, aiosignal, tokenizers, statsmodels, scikit-learn, rich, nvidia-cusolver-cu12, matplotlib, aiohttp, transformers, torch, seaborn, keras, tensorflow, sentence_transformers, datasets, accelerate, tf_keras, evaluate\n",
      "Successfully installed absl-py-2.1.0 accelerate-1.2.1 aiohappyeyeballs-2.4.4 aiohttp-3.11.11 aiosignal-1.3.2 astunparse-1.6.3 contourpy-1.3.1 cycler-0.12.1 datasets-3.2.0 dill-0.3.8 evaluate-0.4.3 filelock-3.16.1 flatbuffers-24.12.23 fonttools-4.55.3 frozenlist-1.5.0 fsspec-2024.9.0 gast-0.6.0 google-pasta-0.2.0 grpcio-1.68.1 h5py-3.12.1 huggingface-hub-0.27.0 joblib-1.4.2 keras-3.7.0 kiwisolver-1.4.8 libclang-18.1.1 markdown-3.7 markdown-it-py-3.0.0 matplotlib-3.10.0 mdurl-0.1.2 ml-dtypes-0.4.1 mpmath-1.3.0 multidict-6.1.0 multiprocess-0.70.16 namex-0.0.8 networkx-3.4.2 numpy-2.0.2 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nccl-cu12-2.21.5 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.4.127 opt-einsum-3.4.0 optree-0.13.1 pandas-2.2.3 patsy-1.0.1 pillow-11.0.0 propcache-0.2.1 protobuf-5.29.2 pyarrow-18.1.0 pytz-2024.2 regex-2024.11.6 rich-13.9.4 safetensors-0.4.5 scikit-learn-1.6.0 scipy-1.14.1 seaborn-0.13.2 sentence_transformers-3.3.1 statsmodels-0.14.4 sympy-1.13.1 tensorboard-2.18.0 tensorboard-data-server-0.7.2 tensorflow-2.18.0 termcolor-2.5.0 tf_keras-2.18.0 threadpoolctl-3.5.0 tokenizers-0.21.0 torch-2.5.1 tqdm-4.67.1 transformers-4.47.1 triton-3.1.0 typing-extensions-4.12.2 tzdata-2024.2 werkzeug-3.1.3 wrapt-1.17.0 xxhash-3.5.0 yarl-1.18.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-31 09:32:24.724267: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1735633944.738701    1333 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1735633944.743083    1333 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-31 09:32:24.759669: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Load libraries\n",
    "!pip install -r /work/NLP_IMDb_Exam/requirements.txt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import datasets\n",
    "import evaluate\n",
    "import seaborn as sns\n",
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, AutoModel\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "from sentence_transformers import SentenceTransformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict= {\n",
    "    1 :{\"name\" : \"MiniLM_L6\",\n",
    "        \"huggingface\" : \"sentence-transformers/all-MiniLM-L6-v2\",},\n",
    "    2 :{\"name\" : \"MPNET_base\",\n",
    "        \"huggingface\" : 'sentence-transformers/all-mpnet-base-v2',},\n",
    "    3 :{\"name\" : \"Instructor\",\n",
    "        \"huggingface\" : \"hkunlp/instructor-large\",},\n",
    "    }\n",
    "# Choose a model for a pseudo-function\n",
    "Chosen_Model = 3\n",
    "\n",
    "data_path = f'../Data/{model_dict[Chosen_Model][\"name\"]}/{model_dict[Chosen_Model][\"name\"]}.csv'\n",
    "active_dataframe = pd.read_csv(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../Data/Instructor/Instructor.csv'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positive_to_negative_vector(Positive, Negative):\n",
    "    \"\"\"\n",
    "    Takes a positive and an negative data point and defines the vector spanning both vectors.\n",
    "    \"\"\"\n",
    "    posneg_vector = Positive.mean().to_frame().T-Negative.mean().to_frame().T\n",
    "    posneg_vector = pd.DataFrame(posneg_vector)\n",
    "    return posneg_vector\n",
    "\n",
    "# Generalise embeddings\n",
    "transformer_model = SentenceTransformer(model_dict[Chosen_Model][\"huggingface\"], device=\"cuda\")\n",
    "if Chosen_Model < 3:\n",
    "    def my_encoder(corpus):\n",
    "        embeddings_df = pd.DataFrame(transformer_model.encode(\n",
    "            corpus))\n",
    "        return embeddings_df\n",
    "\n",
    "if Chosen_Model == 3:\n",
    "    def my_encoder(corpus):\n",
    "        embeddings_df = pd.DataFrame(transformer_model.encode(\n",
    "            corpus,\n",
    "            prompt=\"Represent the movie review for classifying the corresponding movie rating: \"))\n",
    "        return embeddings_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11733, 768)\n",
      "(11733, 768)\n"
     ]
    }
   ],
   "source": [
    "# Define positive and negative average embeddings\n",
    "embeddings = active_dataframe.iloc[:,0:-3]\n",
    "positive = embeddings[active_dataframe['rating'] > 8] #positive ratings defined better ratings than 8 (9, 10)\n",
    "negative = embeddings[active_dataframe['rating'] < 3] #negative ratings defined as worse than 3 (1, 2)\n",
    "PosNeg_vector = positive_to_negative_vector(Positive = positive, Negative = negative)\n",
    "\n",
    "# Determine the minimum length\n",
    "min_length = min(len(positive), len(negative))\n",
    "\n",
    "# Truncate the longer dataframe\n",
    "positive = positive.iloc[:min_length]\n",
    "negative = negative.iloc[:min_length]\n",
    "\n",
    "print(positive.shape)\n",
    "print(negative.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>758</th>\n",
       "      <th>759</th>\n",
       "      <th>760</th>\n",
       "      <th>761</th>\n",
       "      <th>762</th>\n",
       "      <th>763</th>\n",
       "      <th>764</th>\n",
       "      <th>765</th>\n",
       "      <th>766</th>\n",
       "      <th>767</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.004887</td>\n",
       "      <td>-0.011106</td>\n",
       "      <td>0.00481</td>\n",
       "      <td>0.00773</td>\n",
       "      <td>-0.010989</td>\n",
       "      <td>-0.013034</td>\n",
       "      <td>0.006403</td>\n",
       "      <td>-0.002936</td>\n",
       "      <td>0.017168</td>\n",
       "      <td>0.0108</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000735</td>\n",
       "      <td>0.005432</td>\n",
       "      <td>-0.011673</td>\n",
       "      <td>-0.000681</td>\n",
       "      <td>-0.001782</td>\n",
       "      <td>0.013933</td>\n",
       "      <td>-0.002455</td>\n",
       "      <td>0.005975</td>\n",
       "      <td>0.006735</td>\n",
       "      <td>-0.005863</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 768 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0         1        2        3         4         5         6    \\\n",
       "0 -0.004887 -0.011106  0.00481  0.00773 -0.010989 -0.013034  0.006403   \n",
       "\n",
       "        7         8       9    ...       758       759       760       761  \\\n",
       "0 -0.002936  0.017168  0.0108  ...  0.000735  0.005432 -0.011673 -0.000681   \n",
       "\n",
       "        762       763       764       765       766       767  \n",
       "0 -0.001782  0.013933 -0.002455  0.005975  0.006735 -0.005863  \n",
       "\n",
       "[1 rows x 768 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "splits = {'train': 'plain_text/train-00000-of-00001.parquet', 'test': 'plain_text/test-00000-of-00001.parquet'}\n",
    "yelp_reviews = pd.read_parquet(\"hf://datasets/fancyzhx/yelp_polarity/\" + splits[\"train\"])\n",
    "Positive_yelp_Reviews = yelp_reviews[yelp_reviews['label'] == 1].drop(columns = 'label')['text'].values[0:1000]\n",
    "Negative_yelp_Reviews = yelp_reviews[yelp_reviews['label'] == 0].drop(columns = 'label')['text'].values[0:1000]\n",
    "\n",
    "\n",
    "\n",
    "Positive_yelp_embeddings = my_encoder(Positive_yelp_Reviews)\n",
    "Negative_yelp_embeddings = my_encoder(Negative_yelp_Reviews)\n",
    "PosNeg_yelp = positive_to_negative_vector(Positive = Positive_yelp_embeddings, Negative = Negative_yelp_embeddings)\n",
    "PosNeg_yelp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "Positive_GPT_Reviews = np.array([\n",
    "    \"This film redefines the action genre, delivering heart-pounding sequences and jaw-dropping stunts. A must-watch for adrenaline junkies!\",\n",
    "    \"A deeply moving tale that tugs at the heartstrings and leaves you with a renewed sense of hope. Truly unforgettable.\",\n",
    "    \"The visual effects are nothing short of breathtaking. Every frame is a work of art that immerses you completely.\",\n",
    "    \"The cast's chemistry and brilliant performances bring the story to life in the most authentic and engaging way.\",\n",
    "    \"A cinematic masterpiece with impeccable direction that seamlessly blends drama, suspense, and emotion.\",\n",
    "    \"The characters are so well-developed and relatable that you can't help but get invested in their journey.\",\n",
    "    \"An absolute laugh riot from start to finish! The witty dialogue and hilarious antics are sure to leave you in stitches.\",\n",
    "    \"The world-building in this movie is unparalleled. Every detail is carefully crafted, creating a universe you'll never want to leave.\",\n",
    "    \"A powerful and inspiring story that leaves you motivated to chase your dreams and overcome any obstacles.\",\n",
    "    \"The music complements the story beautifully, elevating emotional moments and adding depth to every scene.\",\n",
    "    \"This movie keeps you on the edge of your seat with its clever twists and turns. A gripping ride you won't forget.\",\n",
    "    \"A refreshing take on a familiar theme, offering a perspective that feels both innovative and deeply resonant.\",\n",
    "    \"Perfect for audiences of all ages, this movie delivers laughter, lessons, and love in equal measure.\",\n",
    "    \"The cinematography is a visual feast, capturing both the grandeur of the setting and the intimacy of the characters' emotions.\",\n",
    "    \"A delightful story that warms your heart and reminds you of the simple joys in life.\",\n",
    "    \"An exhilarating journey filled with excitement, danger, and triumph. An epic adventure for the ages.\",\n",
    "    \"The actors' raw and genuine performances make you forget you're watching a movie. Pure artistry.\",\n",
    "    \"This film masterfully combines humor, drama, and action, making it a rollercoaster of emotions from start to finish.\",\n",
    "    \"A vibrant celebration of culture and tradition, beautifully portrayed with authenticity and reverence.\",\n",
    "    \"A film that transcends time with its universal themes and captivating storytelling. Destined to become a classic.\"\n",
    "])\n",
    "\n",
    "Negative_GPT_Reviews = np.array([\n",
    "    \"This movie lacks any sense of direction, leaving the audience confused and frustrated.\",\n",
    "    \"The storyline is painfully predictable, offering nothing new or exciting.\",\n",
    "    \"Poorly written characters make it impossible to care about what happens to them.\",\n",
    "    \"The acting is wooden and emotionless, making every scene feel forced and lifeless.\",\n",
    "    \"A complete waste of stunning visuals due to a hollow and uninspired plot.\",\n",
    "    \"The humor feels forced and falls flat, making the comedy aspect unbearable.\",\n",
    "    \"Pacing issues plague the movie, with some parts dragging endlessly while others feel rushed.\",\n",
    "    \"The dialogue is cringeworthy and unnatural, detracting from the overall experience.\",\n",
    "    \"A disappointing sequel that fails to capture the magic of the original.\",\n",
    "    \"The special effects are overused, overshadowing the weak storytelling.\",\n",
    "    \"This movie tries too hard to be edgy but ends up being obnoxious and shallow.\",\n",
    "    \"The soundtrack is forgettable and adds no value to the film.\",\n",
    "    \"An overstuffed plot with too many subplots that go nowhere.\",\n",
    "    \"The lack of chemistry between the leads makes their relationship unconvincing.\",\n",
    "    \"The ending is abrupt and unsatisfying, leaving more questions than answers.\",\n",
    "    \"An unoriginal rehash of better films, lacking any creativity or fresh ideas.\",\n",
    "    \"The movie’s tone is inconsistent, making it hard to take seriously.\",\n",
    "    \"Unnecessarily long runtime with scenes that add nothing to the story.\",\n",
    "    \"The action sequences are chaotic and poorly choreographed, making them hard to follow.\",\n",
    "    \"An underwhelming experience that fails to leave any lasting impression.\"\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>758</th>\n",
       "      <th>759</th>\n",
       "      <th>760</th>\n",
       "      <th>761</th>\n",
       "      <th>762</th>\n",
       "      <th>763</th>\n",
       "      <th>764</th>\n",
       "      <th>765</th>\n",
       "      <th>766</th>\n",
       "      <th>767</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.024184</td>\n",
       "      <td>-0.010036</td>\n",
       "      <td>-0.000144</td>\n",
       "      <td>-0.005453</td>\n",
       "      <td>-0.001093</td>\n",
       "      <td>-0.013167</td>\n",
       "      <td>0.005935</td>\n",
       "      <td>-0.027534</td>\n",
       "      <td>0.017758</td>\n",
       "      <td>0.022016</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000742</td>\n",
       "      <td>0.002989</td>\n",
       "      <td>-0.005215</td>\n",
       "      <td>0.005257</td>\n",
       "      <td>0.006716</td>\n",
       "      <td>0.006693</td>\n",
       "      <td>-0.003185</td>\n",
       "      <td>0.004785</td>\n",
       "      <td>0.000064</td>\n",
       "      <td>0.00096</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 768 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0         1         2         3         4         5         6    \\\n",
       "0 -0.024184 -0.010036 -0.000144 -0.005453 -0.001093 -0.013167  0.005935   \n",
       "\n",
       "        7         8         9    ...       758       759       760       761  \\\n",
       "0 -0.027534  0.017758  0.022016  ... -0.000742  0.002989 -0.005215  0.005257   \n",
       "\n",
       "        762       763       764       765       766      767  \n",
       "0  0.006716  0.006693 -0.003185  0.004785  0.000064  0.00096  \n",
       "\n",
       "[1 rows x 768 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Positive_GPT_Embeddings= my_encoder(Positive_GPT_Reviews)\n",
    "Negative_GPT_Embeddings= my_encoder(Negative_GPT_Reviews)\n",
    "PosNeg_GPT= positive_to_negative_vector(Positive= Positive_GPT_Embeddings, Negative= Negative_GPT_Embeddings)\n",
    "PosNeg_GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm and GPT:\n",
      "Cosine Similarity: 0.8259100787869493\n",
      "Norm and Yelp:\n",
      "Cosine Similarity: 0.8412883138018823\n",
      "GPT and Yelp:\n",
      "Cosine Similarity: 0.7354161739349365\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "print(\"Norm and GPT:\")\n",
    "#  Assume df1 and df2 are your two DataFrames with shape (1, 768)\n",
    "cosine_sim = cosine_similarity(PosNeg_vector.values, PosNeg_GPT.values)\n",
    "\n",
    "# The result will be a 2D array with shape (1, 1), so extract the single value\n",
    "cosine_similarity_value = cosine_sim[0, 0]\n",
    "\n",
    "print(f\"Cosine Similarity: {cosine_similarity_value}\")\n",
    "\n",
    "\n",
    "print(\"Norm and Yelp:\")\n",
    "#  Assume df1 and df2 are your two DataFrames with shape (1, 768)\n",
    "cosine_sim = cosine_similarity(PosNeg_vector.values, PosNeg_yelp.values)\n",
    "\n",
    "# The result will be a 2D array with shape (1, 1), so extract the single value\n",
    "cosine_similarity_value = cosine_sim[0, 0]\n",
    "\n",
    "\n",
    "print(f\"Cosine Similarity: {cosine_similarity_value}\")\n",
    "\n",
    "print(\"GPT and Yelp:\")\n",
    "#  Assume df1 and df2 are your two DataFrames with shape (1, 768)\n",
    "cosine_sim = cosine_similarity(PosNeg_GPT.values, PosNeg_yelp.values)\n",
    "\n",
    "# The result will be a 2D array with shape (1, 1), so extract the single value\n",
    "cosine_similarity_value = cosine_sim[0, 0]\n",
    "\n",
    "\n",
    "print(f\"Cosine Similarity: {cosine_similarity_value}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def project_matrix_to_vector(matrix, vector):\n",
    "    \"\"\"Compute the projection of a matrix onto the space spanned by the vector\n",
    "    Args:\n",
    "        vector: ndarray of dimension (D, 1), the vector spanning D dimensions that you want to project upon.\n",
    "        matrix: ndarray of dimension (D, M), the matrix consisting of M vectors that you want to map to the subspace spanned by the vector.\n",
    "    \n",
    "    Returns:\n",
    "        p: projection of matrix onto the subspac spanned by the columns of vector; size (D, 1)\n",
    "    \"\"\"\n",
    "    m = matrix.to_numpy() # Turn into a matrix\n",
    "    v = vector.to_numpy()[0] #Turn into a numpy array\n",
    "\n",
    "    # Compute v dot v (denominator)\n",
    "    v_dot_v = np.dot(v, v)\n",
    "\n",
    "    # Compute projection of each row of m onto v\n",
    "    projection = np.outer(np.dot(m, v) / v_dot_v, v)\n",
    "    projection = pd.DataFrame(projection)\n",
    "\n",
    "    return projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def express_matrix_by_vector(matrix, vector):\n",
    "    \"\"\"Compute the projection of a matrix onto the space spanned by the vector\n",
    "    Args:\n",
    "        vector: ndarray of dimension (D, 1), the vector spanning D dimensions that you want to project upon.\n",
    "        matrix: ndarray of dimension (D, M), the matrix consisting of M vectors that you want to map to the subspace spanned by the vector.\n",
    "    \n",
    "    Returns:\n",
    "        projection: projection of matrix onto the subspac spanned by the columns of vector; size (D, 1)\n",
    "        projection_in_1D_subspace: Each embedding projected onto 1 dimensional subspace spanned by input vector.\n",
    "    \"\"\"\n",
    "    unit_vector = vector / np.linalg.norm(vector) # Find the unit vector for interpretatbility by dividing with its norm\n",
    "    projection = project_matrix_to_vector(matrix, vector) # Find projections, so we can find lengths by finding relations in first dimension\n",
    "    projection_in_1D_subspace = projection.iloc[:,0]/unit_vector.iloc[:,0][0] # Location in subspace\n",
    "\n",
    "    return projection, projection_in_1D_subspace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving outputs for future use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save positive negative corrected embedding:\n",
    "projected_variance, projection_in_1D_subspace = express_matrix_by_vector(matrix=embeddings, vector=PosNeg_vector)\n",
    "### \n",
    "posneg_corrected_embeddings = pd.DataFrame(embeddings.to_numpy()-projected_variance.to_numpy())\n",
    "posneg_corrected_embeddings['posneg_subspace'] = projection_in_1D_subspace\n",
    "posneg_corrected_embeddings['rating'] = active_dataframe['rating']\n",
    "posneg_corrected_embeddings['average_rating'] = active_dataframe['average_rating']\n",
    "save_corrected = f'../Data/{model_dict[Chosen_Model][\"name\"]}/{model_dict[Chosen_Model][\"name\"]}_corrected.csv'\n",
    "posneg_corrected_embeddings.to_csv(save_corrected, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save projected data for regression in workbook 07\n",
    "projected_path = f'../Data/{model_dict[Chosen_Model][\"name\"]}/{model_dict[Chosen_Model][\"name\"]}_projected.csv'\n",
    "projected_variance = pd.DataFrame(projected_variance)\n",
    "projected_variance['posneg_subspace']  = projection_in_1D_subspace\n",
    "projected_variance['rating'] = active_dataframe['rating']\n",
    "projected_variance.to_csv(projected_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save GPT positive-negative corrected embedding:\n",
    "\n",
    "# Save positive negative corrected embedding:\n",
    "projected_variance, projection_in_1D_subspace = express_matrix_by_vector(matrix=embeddings, vector=PosNeg_GPT)\n",
    "# projected_variance = project_matrix_to_vector(matrix=embeddings, vector=posneg_vector)\n",
    "posneg_GPT_corrected_embeddings = pd.DataFrame(embeddings.to_numpy()-projected_variance.to_numpy())\n",
    "posneg_GPT_corrected_embeddings['posneg_subspace'] = projection_in_1D_subspace\n",
    "posneg_GPT_corrected_embeddings['rating'] = active_dataframe['rating']\n",
    "posneg_GPT_corrected_embeddings['average_rating'] = active_dataframe['average_rating']\n",
    "save_corrected = f'../Data/{model_dict[Chosen_Model][\"name\"]}/{model_dict[Chosen_Model][\"name\"]}_GPT_corrected.csv'\n",
    "posneg_GPT_corrected_embeddings.to_csv(save_corrected, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save yelp positive-negative corrected embedding:\n",
    "\n",
    "# Save positive negative corrected embedding:\n",
    "projected_variance, projection_in_1D_subspace = express_matrix_by_vector(matrix=embeddings, vector=PosNeg_yelp)\n",
    "# projected_variance = project_matrix_to_vector(matrix=embeddings, vector=posneg_vector)\n",
    "posneg_yelp_corrected_embeddings = pd.DataFrame(embeddings.to_numpy()-projected_variance.to_numpy())\n",
    "posneg_yelp_corrected_embeddings['posneg_subspace'] = projection_in_1D_subspace\n",
    "posneg_yelp_corrected_embeddings['rating'] = active_dataframe['rating']\n",
    "posneg_yelp_corrected_embeddings['average_rating'] = active_dataframe['average_rating']\n",
    "save_corrected = f'../Data/{model_dict[Chosen_Model][\"name\"]}/{model_dict[Chosen_Model][\"name\"]}_yelp_corrected.csv'\n",
    "posneg_yelp_corrected_embeddings.to_csv(save_corrected, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming column names for easier merging\n",
    "PosNeg_GPT.columns = PosNeg_vector.columns\n",
    "PosNeg_yelp.columns  = PosNeg_vector.columns\n",
    "\n",
    "# Concatinate vectors\n",
    "PosNeg_Vectors = pd.concat([PosNeg_vector, PosNeg_GPT, PosNeg_yelp], ignore_index=True)\n",
    "PosNeg_Vectors['Dataset'] = [\"IMDb\", \"GPT\", \"Yelp\"]\n",
    "file_path = f'../Data/{model_dict[Chosen_Model][\"name\"]}/{model_dict[Chosen_Model][\"name\"]}_PosNeg_Vectors.csv'\n",
    "PosNeg_Vectors.to_csv(file_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
